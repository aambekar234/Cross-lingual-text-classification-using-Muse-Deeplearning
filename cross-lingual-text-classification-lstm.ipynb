{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-lingual text Classification with Muse Emebedding (LSTM)\n",
    "## What is covered?\n",
    "1. Load Muse Embiddings\n",
    "2. Data Cleaning and tokenize\n",
    "3. Convert tokenized documents to the embedding vectors\n",
    "3. Building Simple Keras Model\n",
    "4. Test Model on french and German text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "from util import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Muse Embeddings - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utils()\n",
    "word_to_index, index_to_words, word_to_vec_map = util.read_muse_vecs('D:\\Resources\\Muse_Embeddings\\wiki.multi.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997 1997\n",
      "['boring', 'total', 'lack', 'clarity'] 0\n"
     ]
    }
   ],
   "source": [
    "#loading training dataset into a dataframe\n",
    "df = util.read_review_file(\"amazon-dataset/english/books/train.json\")\n",
    "#tokenize loaded dataframe\n",
    "train_set,y = util.tokenize_reviews(df, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "\n",
    "#loading testing dataset into a dataframe\n",
    "df2 = util.read_review_file(\"amazon-dataset/english/books/test.json\")\n",
    "#tokenize loaded dataframe\n",
    "test_set,y2 = util.tokenize_reviews(df2, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "\n",
    "print(len(train_set), len(y))\n",
    "print(train_set[1], y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model - Keras (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#disable warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras Model\n",
    "def my_model(maxLen):\n",
    "    \n",
    "    input_layer = Input(shape = (maxLen,300))\n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(input_layer)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Dense(2)(X)\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    return Model(input=input_layer, output=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get max length doc for padding purpose\n",
    "def get_maxlength_doc(docs):\n",
    "    maxLen = max([len(doc) for doc in docs])\n",
    "    return maxLen\n",
    "\n",
    "def docs_to_vector(docs, vec_map, maxLen):\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        vector = []\n",
    "        for token in doc:\n",
    "            if token.lower() in vec_map:\n",
    "                vector.append(vec_map[token.lower()])\n",
    "            else:\n",
    "                vector.append(vec_map[\"nokey\"])\n",
    "        #padd sequence for max length\n",
    "        pad = maxLen - len(vector)\n",
    "        if pad > 0:\n",
    "            padv = np.zeros((300,),dtype=np.float64)\n",
    "            for i in range(pad):\n",
    "                vector.append(padv)\n",
    "          \n",
    "        #adjust the vector if it greater than max length\n",
    "        if pad < 0:\n",
    "            vector = vector[:pad]\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    return np.array(vectors)\n",
    "\n",
    "#convert lables to one-hot vectors\n",
    "def convert_to_one_hot(y, C):\n",
    "    Y = np.eye(C)[y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "maxLen = get_maxlength_doc(test_set + train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 26, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 26, 128)           219648    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 26, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 351,490\n",
      "Trainable params: 351,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "model = my_model(maxLen)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1997 samples, validate on 1996 samples\n",
      "Epoch 1/4\n",
      "1997/1997 [==============================] - 10s 5ms/step - loss: 0.6172 - acc: 0.6370 - val_loss: 0.5213 - val_acc: 0.7415\n",
      "Epoch 2/4\n",
      "1997/1997 [==============================] - 8s 4ms/step - loss: 0.5083 - acc: 0.7386 - val_loss: 0.5066 - val_acc: 0.7435\n",
      "Epoch 3/4\n",
      "1997/1997 [==============================] - 8s 4ms/step - loss: 0.4952 - acc: 0.7596 - val_loss: 0.5802 - val_acc: 0.7445\n",
      "Epoch 4/4\n",
      "1997/1997 [==============================] - 8s 4ms/step - loss: 0.4638 - acc: 0.7837 - val_loss: 0.5147 - val_acc: 0.7595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x233710a0400>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors = docs_to_vector(train_set, word_to_vec_map, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(np.array(y), C=2)\n",
    "\n",
    "X_test_vectors =  docs_to_vector(test_set, word_to_vec_map, maxLen)\n",
    "Y_test_oh = convert_to_one_hot(np.array(y2), C=2)\n",
    "\n",
    "model.fit(X_train_vectors, Y_train_oh, epochs = 4, batch_size = 32, shuffle=True, validation_data=(X_test_vectors, Y_test_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model on Geraman and French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model with the provided language text\n",
    "def evaluate_model(model, lang=\"de\"):\n",
    "    word_to_index_l, index_to_words_l, word_to_vec_map_l = 0,0,0\n",
    "    df3 = 0\n",
    "    if lang is \"de\":\n",
    "        word_to_index_l, index_to_words_l, word_to_vec_map_l = util.read_muse_vecs('D:\\Resources\\Muse_Embeddings\\wiki.multi.de.vec')\n",
    "        df3 = util.read_review_file(\"amazon-dataset/german/books/test.json\")\n",
    "    elif lang is \"fr\":\n",
    "        word_to_index_l, index_to_words_l, word_to_vec_map_l = util.read_muse_vecs('D:\\Resources\\Muse_Embeddings\\wiki.multi.fr.vec')\n",
    "        df3 = util.read_review_file(\"amazon-dataset/french/books/test.json\")\n",
    "    \n",
    "    test_set_l,y3 = util.tokenize_reviews(df3, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "    \n",
    "    X_test_l_vectors =  docs_to_vector(test_set_l, word_to_vec_map_l, maxLen)\n",
    "    Y_test_l_oh = convert_to_one_hot(np.array(y3), C=2)\n",
    "    \n",
    "    loss,acc = model.evaluate(x=X_test_l_vectors, y=Y_test_l_oh, batch_size=32, verbose=1)\n",
    "    return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model on German Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 3s 1ms/step\n",
      "0.7015\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, lang=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model on French Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 3s 1ms/step\n",
      "0.679\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, lang=\"fr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
