{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-lingual text Classification with Muse Emebedding\n",
    "## What is covered?\n",
    "1. Load Muse Embiddings\n",
    "2. Data Cleaning\n",
    "3. Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "from util import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Muse Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utils()\n",
    "word_to_index, index_to_words, word_to_vec_map = util.read_muse_vecs('D:\\Resources\\Muse_Embeddings\\wiki.multi.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997 1997\n",
      "['boring', 'total', 'lack', 'clarity'] 0\n",
      "1996 1996\n",
      "['refinement', 'needed'] 0\n"
     ]
    }
   ],
   "source": [
    "df = util.read_review_file(\"amazon-dataset/english/books/train.json\")\n",
    "train_set,y = util.tokenize_reviews(df, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "\n",
    "df2 = util.read_review_file(\"amazon-dataset/english/books/test.json\")\n",
    "test_set,y2 = util.tokenize_reviews(df2, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "\n",
    "print(len(train_set), len(y))\n",
    "print(train_set[1], y[1])\n",
    "\n",
    "print(len(test_set), len(y2))\n",
    "print(test_set[1], y2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building - Keras LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts list of tokenize docs to list of glove indices \n",
    "#max_len is a length of doc with maximum length, all other smaller sentences will be padded with 0\n",
    "def sentences_to_indices(docs, word_to_index, max_len):\n",
    "    \n",
    "    m = len(docs)                                \n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in docs[i]:\n",
    "            if w.lower() in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w.lower()]\n",
    "            else:\n",
    "                X_indices[i, j] = word_to_index[\"nokey\"]\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 82344. 104225.  63850.]\n",
      " [ 77419. 175887.      0.]]\n"
     ]
    }
   ],
   "source": [
    "docs = [[\"I\", \"love\", \"football\"], [\"hello\", \"there\"]]\n",
    "indices = sentences_to_indices(docs, word_to_index, max_len=3)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = 300      \n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        if len(word_to_vec_map[word]) != 300:\n",
    "            print(word)\n",
    "        else:    \n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim,trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    \n",
    "    sentence_indices = Input(input_shape, dtype = 'int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices) \n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Dense(2)(X)\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    return Model(input=sentence_indices, output=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_vector(docs, vec_map):\n",
    "    vectors = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        vector = np.zeros((300,), dtype=np.float64)\n",
    "        for token in doc:\n",
    "            if token.lower() in vec_map:\n",
    "                vector += vec_map[token.lower()]\n",
    "            else:\n",
    "                vector += vec_map[\"nokey\"]\n",
    "        vector /= len(doc)\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model2():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    X = Dense(128)(input_layer)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    X = Dense(2)(X)\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    return Model(input=input_layer, output=X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 09:52:55.508239 15384 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 09:52:55.527189 15384 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 09:52:55.532203 15384 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  \n",
      "W0714 09:52:55.563162 15384 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 09:52:55.585307 15384 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "18\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 38,786\n",
      "Trainable params: 38,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get length of longest document\n",
    "maxLen = max([len(doc) for doc in train_set])\n",
    "print(maxLen)\n",
    "\n",
    "maxLen2 = max([len(doc) for doc in test_set])\n",
    "print(maxLen2)\n",
    "\n",
    "model2 = my_model2()\n",
    "model2.summary()\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 09:53:00.106420 15384 deprecation.py:323] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 09:53:00.148077 15384 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1997 samples, validate on 1996 samples\n",
      "Epoch 1/4\n",
      "1997/1997 [==============================] - 4s 2ms/step - loss: 0.6373 - acc: 0.6690 - val_loss: 0.5701 - val_acc: 0.7174\n",
      "Epoch 2/4\n",
      "1997/1997 [==============================] - 0s 145us/step - loss: 0.5291 - acc: 0.7536 - val_loss: 0.5090 - val_acc: 0.7545\n",
      "Epoch 3/4\n",
      "1997/1997 [==============================] - 0s 143us/step - loss: 0.4838 - acc: 0.7566 - val_loss: 0.4935 - val_acc: 0.7655\n",
      "Epoch 4/4\n",
      "1997/1997 [==============================] - 0s 128us/step - loss: 0.4631 - acc: 0.7747 - val_loss: 0.4921 - val_acc: 0.7610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ceaf159860>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_one_hot(y, C):\n",
    "    Y = np.eye(C)[y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "\n",
    "X_train_indices = docs_to_vector(train_set, word_to_vec_map)\n",
    "Y_train_oh = convert_to_one_hot(np.array(y), C=2)\n",
    "\n",
    "X_test_indices =  docs_to_vector(test_set, word_to_vec_map)\n",
    "Y_test_oh = convert_to_one_hot(np.array(y2), C=2)\n",
    "\n",
    "model2.fit(X_train_indices, Y_train_oh, epochs = 4, batch_size = 32, shuffle=True, validation_data=(X_test_indices, Y_test_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 01:30:33.551189 10000 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 01:30:33.567146 10000 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 01:30:34.072847 10000 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 01:30:34.080825 10000 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0714 01:30:34.081822 10000 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0714 01:30:36.732684 10000 deprecation.py:506] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 26, 300)           59997000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 26, 128)           219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 26, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 60,348,490\n",
      "Trainable params: 351,490\n",
      "Non-trainable params: 59,997,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = my_model((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 01:30:36.947109 10000 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(y, C):\n",
    "    Y = np.eye(C)[y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "\n",
    "X_train_indices = sentences_to_indices(train_set, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(np.array(y), C=2)\n",
    "\n",
    "X_test_indices =  sentences_to_indices(test_set, word_to_index, maxLen)\n",
    "Y_test_oh = convert_to_one_hot(np.array(y2), C=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 01:30:37.102441 10000 deprecation.py:323] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1997 samples, validate on 1996 samples\n",
      "Epoch 1/4\n",
      "1997/1997 [==============================] - 9s 5ms/step - loss: 0.6521 - acc: 0.5934 - val_loss: 0.5597 - val_acc: 0.7290\n",
      "Epoch 2/4\n",
      "1997/1997 [==============================] - 8s 4ms/step - loss: 0.5216 - acc: 0.7476 - val_loss: 0.5336 - val_acc: 0.7570\n",
      "Epoch 3/4\n",
      "1997/1997 [==============================] - 8s 4ms/step - loss: 0.4825 - acc: 0.7666 - val_loss: 0.5543 - val_acc: 0.7365\n",
      "Epoch 4/4\n",
      "1997/1997 [==============================] - 8s 4ms/step - loss: 0.4644 - acc: 0.7867 - val_loss: 0.5241 - val_acc: 0.7465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e905539390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 4, batch_size = 32, shuffle=True, validation_data=(X_test_indices, Y_test_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model on German text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index_de, index_to_words_de, word_to_vec_map_de = util.read_muse_vecs('D:\\Resources\\Muse_Embeddings\\wiki.multi.de.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "['e', 'musste', 'ja', 'kommen', '...', '...'] 0\n"
     ]
    }
   ],
   "source": [
    "df3 = util.read_review_file(\"amazon-dataset/german/books/test.json\")\n",
    "test_set_de,y3 = util.tokenize_reviews(df3, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "\n",
    "print(len(test_set_de), len(y3))\n",
    "print(test_set_de[1], y3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00653661  0.02699564  0.00302272  0.02870599 -0.01389753 -0.00338064\n",
      "  0.00559786 -0.03346153  0.00070467  0.02043842 -0.03139933  0.00822137\n",
      " -0.00539226 -0.04022262  0.0118907  -0.05106944 -0.01170563  0.02477052\n",
      "  0.01984421  0.02599529  0.01040407  0.00016941 -0.03355211 -0.0111978\n",
      " -0.02845445 -0.02703157 -0.00537891 -0.01917961  0.01048362  0.03906411\n",
      " -0.03453014  0.04529503 -0.06386306  0.00661982  0.00041111 -0.01415118\n",
      "  0.00294962 -0.02498741  0.01491031 -0.0217604  -0.00218333  0.000658\n",
      " -0.06380795 -0.00809302 -0.01447022  0.00931652  0.02222784  0.01318663\n",
      "  0.04336886 -0.02933589  0.01648976 -0.03507637 -0.00878142 -0.01914704\n",
      " -0.01513701  0.02098027 -0.00102045 -0.02033681  0.0313291   0.05830464\n",
      " -0.02235418 -0.02987555  0.05824889 -0.00662939 -0.03876732 -0.02090494\n",
      "  0.00667941  0.03024812 -0.01016959  0.01416782  0.00364605  0.00987893\n",
      "  0.02990993 -0.03740573 -0.03310736  0.04370545  0.04237576  0.02729444\n",
      "  0.01277791 -0.00365579  0.01817824 -0.02823714  0.01939486 -0.01582745\n",
      "  0.00432047 -0.00511413 -0.01910357 -0.00049369  0.00282208 -0.00848217\n",
      "  0.01100369  0.03101916  0.04454869 -0.03362621  0.0433864   0.01528661\n",
      "  0.01503182  0.01504464 -0.0465067   0.04187971  0.0137816  -0.01266607\n",
      "  0.01328152  0.02598552 -0.00616843 -0.00828336 -0.02676361 -0.01258241\n",
      "  0.02938838 -0.01900585 -0.03951161 -0.02505302 -0.02042982 -0.00375069\n",
      "  0.02501858 -0.03208694 -0.00050111  0.00968848  0.00389616  0.05946127\n",
      "  0.08274727 -0.02358264 -0.0132948   0.06450462  0.01870106  0.00928849\n",
      "  0.02214759  0.00831667 -0.01056951  0.06305416 -0.01969931  0.00096903\n",
      " -0.0283825  -0.00347493  0.00293071  0.00923741  0.01499553 -0.00326863\n",
      "  0.01236164  0.06027283 -0.01565069 -0.03499843  0.01190774  0.02101813\n",
      " -0.02864344  0.06124765 -0.04504253 -0.01960471 -0.01075707  0.0315086\n",
      "  0.01374354  0.01559133 -0.02661718  0.00030512 -0.02376884 -0.0409617\n",
      "  0.05299109 -0.03067869  0.05132768  0.03156861 -0.01989278  0.02919372\n",
      " -0.08501311  0.0106801  -0.00663813 -0.01580702 -0.0094326  -0.0086749\n",
      "  0.01665383 -0.00487445 -0.03059663 -0.02659182 -0.06179163  0.01341163\n",
      " -0.03162787  0.05199131 -0.04335654  0.01333635 -0.01679567 -0.03812343\n",
      "  0.01396051 -0.05225872  0.00775903  0.005847    0.00035108  0.02405878\n",
      "  0.01549114 -0.05463717 -0.00887059  0.03363335 -0.02588759 -0.07623733\n",
      "  0.05164382  0.02166352 -0.05211653 -0.0374135   0.04958923  0.04496579\n",
      " -0.08605634 -0.03051829 -0.00281214  0.0289418   0.05662774  0.00971813\n",
      "  0.01930274 -0.03834669 -0.0223852  -0.01144486  0.04519054  0.00142058\n",
      "  0.12403473 -0.00611525  0.01547584  0.00459723 -0.00089561 -0.02781303\n",
      "  0.00674034 -0.06094504 -0.06817844  0.00792243 -0.01120197 -0.0383873\n",
      " -0.02712386  0.00312107  0.01297961 -0.01481203  0.02366663 -0.02353044\n",
      "  0.0216687  -0.00113283 -0.01430424  0.01493139 -0.01481862 -0.0419423\n",
      " -0.02879239  0.00095738  0.02640527 -0.03468196 -0.0049251  -0.01063376\n",
      " -0.03521703 -0.02800995  0.03919547 -0.03013739  0.02162305  0.05061094\n",
      "  0.01641524  0.00016765 -0.01615854  0.00269541  0.00196999 -0.0373278\n",
      " -0.08349846 -0.0045385   0.03522554 -0.00478847  0.00234401 -0.01437719\n",
      "  0.00355306  0.02226617 -0.00081774  0.01904999 -0.01340849 -0.00852242\n",
      " -0.0038637  -0.00446179 -0.00375685  0.01385047 -0.02330388 -0.03613289\n",
      " -0.07273967 -0.01164754  0.0244628   0.0098311  -0.0229429  -0.03284558\n",
      "  0.06544712  0.02049752  0.00578433 -0.02226447  0.02194492  0.02396675\n",
      " -0.00848083  0.0017787   0.02043299 -0.01051864  0.00765349  0.01072755\n",
      "  0.0036783   0.04804143 -0.03010326 -0.00650684 -0.00181873 -0.02405127\n",
      " -0.02221799 -0.0320406  -0.02749145 -0.00015176  0.04766239  0.0698727 ]\n"
     ]
    }
   ],
   "source": [
    "X_test_de_indices =  docs_to_vector(test_set_de, word_to_vec_map_de)\n",
    "Y_test_de_oh = convert_to_one_hot(np.array(y3), C=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 143us/step\n",
      "0.681\n",
      "199982\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model2.evaluate(x=X_test_de_indices, y=Y_test_de_oh, batch_size=32, verbose=1)\n",
    "print(acc)\n",
    "print(len(index_to_words_de))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model on French text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index_fr, index_to_words_fr, word_to_vec_map_fr = util.read_muse_vecs('D:\\Resources\\Muse_Embeddings\\wiki.multi.fr.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "['super', 'recettes', 'faciles', 'à', 'réaliser'] 1\n"
     ]
    }
   ],
   "source": [
    "df4 = util.read_review_file(\"amazon-dataset/french/books/test.json\")\n",
    "test_set_fr,y4 = util.tokenize_reviews(df4, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "\n",
    "print(len(test_set_fr), len(y4))\n",
    "print(test_set_fr[1], y4[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_de_indices =  docs_to_vector(test_set_fr, word_to_vec_map_fr)\n",
    "Y_test_de_oh = convert_to_one_hot(np.array(y4), C=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 145us/step\n",
      "0.7245\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index_to_words_de' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-40d46178b83a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_test_de_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_test_de_oh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_to_words_de\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'index_to_words_de' is not defined"
     ]
    }
   ],
   "source": [
    "loss,acc = model2.evaluate(x=X_test_de_indices, y=Y_test_de_oh, batch_size=32, verbose=1)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
