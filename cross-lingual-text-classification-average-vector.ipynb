{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-lingual text Classification with Muse Emebedding (Vector Average)\n",
    "## What is covered?\n",
    "1. Load Muse Embiddings\n",
    "2. Data Cleaning and tokenize\n",
    "3. Convert tokenized documents to the embedding vectors calulted by averaging\n",
    "3. Building Simple Keras Model\n",
    "4. Test Model on french and German text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "from util import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Muse Embeddings - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utils()\n",
    "word_to_index, index_to_words, word_to_vec_map = util.read_muse_vecs('D:\\Resources\\Muse_Embeddings\\wiki.multi.en.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997 1997\n",
      "['boring', 'total', 'lack', 'clarity'] 0\n"
     ]
    }
   ],
   "source": [
    "#loading training dataset into a dataframe\n",
    "df = util.read_review_file(\"amazon-dataset/english/books/train.json\")\n",
    "#tokenize loaded dataframe\n",
    "train_set,y = util.tokenize_reviews(df, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "\n",
    "#loading testing dataset into a dataframe\n",
    "df2 = util.read_review_file(\"amazon-dataset/english/books/test.json\")\n",
    "#tokenize loaded dataframe\n",
    "test_set,y2 = util.tokenize_reviews(df2, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "\n",
    "print(len(train_set), len(y))\n",
    "print(train_set[1], y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model - Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#disable warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras Model\n",
    "def my_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    X = Dense(128)(input_layer)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    X = Dense(2)(X)\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    return Model(input=input_layer, output=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tokenized docs to vector embeddings by averaging\n",
    "def docs_to_vector(docs, vec_map):\n",
    "    vectors = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        vector = np.zeros((300,), dtype=np.float64)\n",
    "        for token in doc:\n",
    "            if token.lower() in vec_map:\n",
    "                vector += vec_map[token.lower()]\n",
    "            else:\n",
    "                vector += vec_map[\"nokey\"]\n",
    "        vector /= len(doc)\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "#convert lables to one-hot vectors\n",
    "def convert_to_one_hot(y, C):\n",
    "    Y = np.eye(C)[y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 38,786\n",
      "Trainable params: 38,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = my_model()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1997 samples, validate on 1996 samples\n",
      "Epoch 1/4\n",
      "1997/1997 [==============================] - 1s 340us/step - loss: 0.6373 - acc: 0.6690 - val_loss: 0.5701 - val_acc: 0.7174\n",
      "Epoch 2/4\n",
      "1997/1997 [==============================] - 0s 134us/step - loss: 0.5291 - acc: 0.7536 - val_loss: 0.5090 - val_acc: 0.7545\n",
      "Epoch 3/4\n",
      "1997/1997 [==============================] - 0s 130us/step - loss: 0.4838 - acc: 0.7566 - val_loss: 0.4935 - val_acc: 0.7655\n",
      "Epoch 4/4\n",
      "1997/1997 [==============================] - 0s 132us/step - loss: 0.4631 - acc: 0.7747 - val_loss: 0.4921 - val_acc: 0.7610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab2b13fdd8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors = docs_to_vector(train_set, word_to_vec_map)\n",
    "Y_train_oh = convert_to_one_hot(np.array(y), C=2)\n",
    "\n",
    "X_test_vectors =  docs_to_vector(test_set, word_to_vec_map)\n",
    "Y_test_oh = convert_to_one_hot(np.array(y2), C=2)\n",
    "\n",
    "model.fit(X_train_vectors, Y_train_oh, epochs = 4, batch_size = 32, shuffle=True, validation_data=(X_test_vectors, Y_test_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model on Geraman and French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model with the provided language text\n",
    "def evaluate_model(model, lang=\"de\"):\n",
    "    word_to_index_l, index_to_words_l, word_to_vec_map_l = 0,0,0\n",
    "    df3 = 0\n",
    "    if lang is \"de\":\n",
    "        word_to_index_l, index_to_words_l, word_to_vec_map_l = util.read_muse_vecs('D:\\Resources\\Muse_Embeddings\\wiki.multi.de.vec')\n",
    "        df3 = util.read_review_file(\"amazon-dataset/german/books/test.json\")\n",
    "    if lang is \"fr\":\n",
    "        word_to_index_l, index_to_words_l, word_to_vec_map_l = util.read_muse_vecs('D:\\Resources\\Muse_Embeddings\\wiki.multi.fr.vec')\n",
    "        df3 = util.read_review_file(\"amazon-dataset/french/books/test.json\")\n",
    "    \n",
    "    test_set_l,y3 = util.tokenize_reviews(df3, keep_text=False, stemming=False, keep_punctuation=True)\n",
    "    \n",
    "    X_test_l_vectors =  docs_to_vector(test_set_l, word_to_vec_map_l)\n",
    "    Y_test_l_oh = convert_to_one_hot(np.array(y3), C=2)\n",
    "    \n",
    "    loss,acc = model.evaluate(x=X_test_l_vectors, y=Y_test_l_oh, batch_size=32, verbose=1)\n",
    "    return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model on German Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 103us/step\n",
      "0.705\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, lang=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model on French Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 99us/step\n",
      "0.7245\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, lang=\"fr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
